{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n",
      "There are 0 total dog images.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# load filenames for human and dog images\n",
    "dog_files = np.array(glob(\"data/dog_images/*/*/*\"))\n",
    "\n",
    "# print number of images in each dataset\n",
    "print('There are %d total dog images.' % len(dog_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(244),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.RandomRotation(20),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                         [0.229, 0.224, 0.225])]) \n",
    "test_transform = transforms.Compose([transforms.Resize(225),\n",
    "                                     transforms.CenterCrop(244),\n",
    "                                    transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                         [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dset = datasets.ImageFolder('/data/dog_images/train/', transform=train_transform)\n",
    "val_dset = datasets.ImageFolder('/data/dog_images/valid/', transform=test_transform)\n",
    "test_dset = datasets.ImageFolder('/data/dog_images/test/', transform=test_transform)\n",
    "\n",
    "loaders_scratch = {\n",
    "    'train' : torch.utils.data.DataLoader(train_dset, batch_size=32, shuffle=True),\n",
    "    'test' : torch.utils.data.DataLoader(test_dset, batch_size=32, shuffle=True),\n",
    "    'valid' : torch.utils.data.DataLoader(val_dset, batch_size=32, shuffle=True)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bc1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bc2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bc3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bc4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=28800, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=133, bias=True)\n",
      "  (bc_fc1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bc_fc2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout2d(p=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## layers of a CNN\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,16,5,padding=2)\n",
    "        self.conv2 = nn.Conv2d(16,32,3,padding=1)\n",
    "        self.conv3 = nn.Conv2d(32,64,3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(64,128,3,padding=1)\n",
    "\n",
    "        self.bc1 = nn.BatchNorm2d(16)\n",
    "        self.bc2 = nn.BatchNorm2d(32)\n",
    "        self.bc3 = nn.BatchNorm2d(64)\n",
    "        self.bc4 = nn.BatchNorm2d(128)   \n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.fc1 = nn.Linear(15*15*128,1024)\n",
    "        self.fc2 = nn.Linear(1024,512)\n",
    "        self.fc3 = nn.Linear(512,133)\n",
    "        \n",
    "        self.bc_fc1 = nn.BatchNorm1d(1024)\n",
    "        self.bc_fc2 = nn.BatchNorm1d(512)\n",
    "    \n",
    "        self.dropout = nn.Dropout2d(p=.5)\n",
    "    def forward(self, x):\n",
    "        ## forward behavior\n",
    "        x = self.bc1(self.pool(F.relu(self.conv1(x))))\n",
    "        x = self.bc2(self.pool(F.relu(self.conv2(x))))        \n",
    "        x = self.bc3(self.pool(F.relu(self.conv3(x))))\n",
    "        x = self.bc4(self.pool(F.relu(self.conv4(x))))\n",
    "\n",
    "        x = self.dropout(x.view(x.shape[0], -1))\n",
    "        \n",
    "        x = self.dropout(self.bc_fc1(F.relu(self.fc1(x))))\n",
    "        x = self.dropout(self.bc_fc2(F.relu(self.fc2(x))))\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "  \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# instantiate the CNN\n",
    "model_scratch = Net()\n",
    "print(model_scratch)\n",
    "\n",
    "# # move tensors to GPU if CUDA is available\n",
    "if True:\n",
    "    model_scratch.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion_scratch = nn.NLLLoss()\n",
    "\n",
    "optimizer_scratch = optim.Adam(model_scratch.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 4.826334 \tValidation Loss: 3.955903\n",
      "Epoch: 2 \tTraining Loss: 4.826026 \tValidation Loss: 3.860584\n",
      "Saving Model\n",
      "Epoch: 3 \tTraining Loss: 4.833354 \tValidation Loss: 3.835989\n",
      "Saving Model\n",
      "Epoch: 4 \tTraining Loss: 4.783208 \tValidation Loss: 3.788778\n",
      "Saving Model\n",
      "Epoch: 5 \tTraining Loss: 4.786644 \tValidation Loss: 3.750323\n",
      "Saving Model\n",
      "Epoch: 6 \tTraining Loss: 4.827653 \tValidation Loss: 3.735185\n",
      "Saving Model\n",
      "Epoch: 7 \tTraining Loss: 4.818615 \tValidation Loss: 3.694227\n",
      "Saving Model\n",
      "Epoch: 8 \tTraining Loss: 4.772934 \tValidation Loss: 3.711705\n",
      "Epoch: 9 \tTraining Loss: 4.801313 \tValidation Loss: 3.708015\n",
      "Epoch: 10 \tTraining Loss: 4.827687 \tValidation Loss: 3.753609\n",
      "Epoch: 11 \tTraining Loss: 4.785099 \tValidation Loss: 3.699762\n",
      "Epoch: 12 \tTraining Loss: 4.777802 \tValidation Loss: 3.750700\n",
      "Epoch: 13 \tTraining Loss: 4.781232 \tValidation Loss: 3.611180\n",
      "Saving Model\n",
      "Epoch: 14 \tTraining Loss: 4.793339 \tValidation Loss: 3.635408\n",
      "Epoch: 15 \tTraining Loss: 4.785834 \tValidation Loss: 3.609788\n",
      "Saving Model\n",
      "Epoch: 16 \tTraining Loss: 4.726502 \tValidation Loss: 3.524635\n",
      "Saving Model\n",
      "Epoch: 17 \tTraining Loss: 4.800368 \tValidation Loss: 3.599520\n",
      "Epoch: 18 \tTraining Loss: 4.769266 \tValidation Loss: 3.590260\n",
      "Epoch: 19 \tTraining Loss: 4.691786 \tValidation Loss: 3.510606\n",
      "Saving Model\n",
      "Epoch: 20 \tTraining Loss: 4.758739 \tValidation Loss: 3.544174\n",
      "Epoch: 21 \tTraining Loss: 4.688085 \tValidation Loss: 3.591688\n",
      "Epoch: 22 \tTraining Loss: 4.680574 \tValidation Loss: 3.498688\n",
      "Saving Model\n",
      "Epoch: 23 \tTraining Loss: 4.692735 \tValidation Loss: 3.578859\n",
      "Epoch: 24 \tTraining Loss: 4.711243 \tValidation Loss: 3.573285\n",
      "Epoch: 25 \tTraining Loss: 4.694627 \tValidation Loss: 3.536111\n",
      "Epoch: 26 \tTraining Loss: 4.746087 \tValidation Loss: 3.473852\n",
      "Saving Model\n",
      "Epoch: 27 \tTraining Loss: 4.655578 \tValidation Loss: 3.462878\n",
      "Saving Model\n",
      "Epoch: 28 \tTraining Loss: 4.698240 \tValidation Loss: 3.480565\n",
      "Epoch: 29 \tTraining Loss: 4.683486 \tValidation Loss: 3.470301\n",
      "Epoch: 30 \tTraining Loss: 4.664516 \tValidation Loss: 3.464248\n",
      "Epoch: 31 \tTraining Loss: 4.659087 \tValidation Loss: 3.437220\n",
      "Saving Model\n",
      "Epoch: 32 \tTraining Loss: 4.702716 \tValidation Loss: 3.412012\n",
      "Saving Model\n",
      "Epoch: 33 \tTraining Loss: 4.686810 \tValidation Loss: 3.497362\n",
      "Epoch: 34 \tTraining Loss: 4.600335 \tValidation Loss: 3.349760\n",
      "Saving Model\n",
      "Epoch: 35 \tTraining Loss: 4.645170 \tValidation Loss: 3.474122\n",
      "Epoch: 36 \tTraining Loss: 4.621978 \tValidation Loss: 3.390508\n",
      "Epoch: 37 \tTraining Loss: 4.679235 \tValidation Loss: 3.445867\n",
      "Epoch: 38 \tTraining Loss: 4.663040 \tValidation Loss: 3.381190\n",
      "Epoch: 39 \tTraining Loss: 4.678441 \tValidation Loss: 3.409582\n",
      "Epoch: 40 \tTraining Loss: 4.653763 \tValidation Loss: 3.378394\n",
      "Epoch: 41 \tTraining Loss: 4.635642 \tValidation Loss: 3.388540\n",
      "Epoch: 42 \tTraining Loss: 4.580755 \tValidation Loss: 3.312202\n",
      "Saving Model\n",
      "Epoch: 43 \tTraining Loss: 4.624352 \tValidation Loss: 3.318330\n",
      "Epoch: 44 \tTraining Loss: 4.614851 \tValidation Loss: 3.361203\n",
      "Epoch: 45 \tTraining Loss: 4.641850 \tValidation Loss: 3.287591\n",
      "Saving Model\n",
      "Epoch: 46 \tTraining Loss: 4.582147 \tValidation Loss: 3.368081\n",
      "Epoch: 47 \tTraining Loss: 4.546171 \tValidation Loss: 3.311322\n",
      "Epoch: 48 \tTraining Loss: 4.594288 \tValidation Loss: 3.290295\n",
      "Epoch: 49 \tTraining Loss: 4.567544 \tValidation Loss: 3.267587\n",
      "Saving Model\n",
      "Epoch: 50 \tTraining Loss: 4.594396 \tValidation Loss: 3.283331\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            \n",
    "            loss = criterion(output,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "      \n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)            \n",
    "            valid_loss += ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Saving Model')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "# train the model\n",
    "model_scratch = train(50, loaders_scratch, model_scratch, optimizer_scratch, \n",
    "                      criterion_scratch, True, 'model_scratch.pt')\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model_scratch.load_state_dict(torch.load('model_scratch.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.208596\n",
      "\n",
      "\n",
      "Test Accuracy: 30% (257/836)\n"
     ]
    }
   ],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "    \n",
    "test(loaders_scratch, model_scratch, criterion_scratch, True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
